{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing around with reinforcement learning and multi-agent trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "from itertools import product, combinations, permutations\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_table:\n",
    "\n",
    "    def __init__(self,\n",
    "                 Np = 31,\n",
    "                 Nv = 10, \n",
    "                 Nx = 15,\n",
    "                 initial_values = None):\n",
    "\n",
    "        self.Np = range(Np)\n",
    "        self.Nv = range(Nv)\n",
    "\n",
    "        states = list(product(self.Np, self.Nv))\n",
    "\n",
    "        self.Q = {s:np.zeros(Nx) for s in states}\n",
    "\n",
    "        # if initial_values is not None:\n",
    "        #     self.Q = initial_values\n",
    "        \n",
    "    def get_Q_value(self, state, action):\n",
    "        return self.Q[state][action]\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "    def get_best_value(self, state):\n",
    "        return np.max(self.Q[state])\n",
    "\n",
    "    def update(self, state, action, value):\n",
    "        self.Q[state][action] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informed (potentially) Collusive Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformedAgent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 Np = 31,\n",
    "                 Nv = 10,\n",
    "                 Nx = 15, \n",
    "                 \n",
    "                 rho = 0.95, \n",
    "                 alpha = 0.01, \n",
    "                 beta = 1e-5):\n",
    "        \n",
    "        self.n_actions = Nx\n",
    "        self.n_states = Np * Nv\n",
    "\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.Q = Q_table(Np = Np, Nv = Nv, Nx = Nx)\n",
    "\n",
    "        self.state_count = defaultdict(int)\n",
    "    \n",
    "    def get_epsilon(self, state):\n",
    "        v = self.state_count[state]\n",
    "        self.state_count[state] += 1\n",
    "        return np.exp(-self.beta * v)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        epsilon = self.get_epsilon(state)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return self.Q.get_best_action(state)\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        learning = self.alpha * (reward + self.rho * self.Q.get_best_value(next_state))\n",
    "        memory = (1 - self.alpha) * self.Q.get_Q_value(state, action)\n",
    "        value = learning + memory\n",
    "        self.Q.update(state, action, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferred Habitat Investor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferredHabitatAgent:\n",
    "\n",
    "    def __init__(self, xi = 500, v_bar = 1):\n",
    "        self.xi = xi\n",
    "        self.v_bar = v_bar\n",
    "\n",
    "    def get_action(self, pt):\n",
    "        z = -self.xi * (pt - self.v_bar)\n",
    "        return z\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Market Makers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircularBuffer:\n",
    "    \"\"\"\n",
    "    Circular buffer for storing historical data.\n",
    "    \"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.buffer = np.zeros(size)\n",
    "        self.index = 0\n",
    "\n",
    "    def add(self, value):\n",
    "        self.buffer[self.index] = value\n",
    "        self.index = (self.index + 1) % self.size\n",
    "\n",
    "    def get(self):\n",
    "        return np.concatenate((self.buffer[self.index:], self.buffer[:self.index]))\n",
    "\n",
    "class AdaptiveMarketMaker:\n",
    "\n",
    "    def __init__(self, theta, Tm):\n",
    "        self.theta = theta\n",
    "        self.Tm = Tm\n",
    "\n",
    "        self.vars_ = ['v','p','z','y']\n",
    "        self.historical_data = {var: CircularBuffer(size = self.Tm) for var in self.vars_}\n",
    "\n",
    "    def OLS(self, y, X):\n",
    "        \"\"\"\n",
    "        Perform Ordinary Least Squares (OLS) regression.\n",
    "        Parameters:\n",
    "        y (CircularBuffer): The dependent variable.\n",
    "        X (CircularBuffer): The independent variable(s).\n",
    "        Returns:\n",
    "        coef_ (ndarray): The estimated coefficients for the linear regression model.\n",
    "        \"\"\"\n",
    "        y = y.get()\n",
    "        X = X.get()\n",
    "        \n",
    "        X = np.vstack([X, np.ones(len(X))]).T\n",
    "        coef_, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "        return coef_\n",
    "    \n",
    "    def determine_price(self, yt):\n",
    "        \"\"\"\n",
    "        Determines the price based on historical data and a given input.\n",
    "        This method uses Ordinary Least Squares (OLS) regression to calculate\n",
    "        coefficients from historical data and then uses these coefficients to\n",
    "        determine the price for a given input `yt`.\n",
    "        Parameters:\n",
    "        yt (float): The input value for which the price needs to be determined.\n",
    "        Returns:\n",
    "        float: The determined price based on the input `yt`.\n",
    "        \"\"\"\n",
    "\n",
    "        xi_1, xi_0 = self.OLS(self.historical_data['z'], self.historical_data['p'])\n",
    "        gamma_1, gamma_0 = self.OLS(self.historical_data['v'], self.historical_data['y'])\n",
    "        lambda_ = (xi_1 + self.theta * gamma_1) / (xi_1**2 + self.theta)\n",
    "        price = gamma_0 + lambda_ * yt\n",
    "        return price\n",
    "    \n",
    "    def update(self, vt, pt, zt, yt):\n",
    "        \"\"\"\n",
    "        Updates the historical data with the given values.\n",
    "        Parameters:\n",
    "        vt (float): The value of `v` at time `t`.\n",
    "        pt (float): The value of `p` at time `t`.\n",
    "        zt (float): The value of `z` at time `t`.\n",
    "        yt (float): The value of `y` at time `t`.\n",
    "        \"\"\"\n",
    "        for var, value in zip(self.vars_, [vt, pt, zt, yt]):\n",
    "            self.historical_data[var].add(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Trader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseTrader:\n",
    "    def __init__(self, sigma = 0.1):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def get_action(self):\n",
    "        return np.random.normal(scale = self.sigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
