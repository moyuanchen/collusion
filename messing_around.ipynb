{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing around with reinforcement learning and multi-agent trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_table:\n",
    "\n",
    "    def __init__(self,\n",
    "                 size, \n",
    "                 initial_values = None):\n",
    "\n",
    "        \n",
    "        self.size = size\n",
    "        self.Q = np.random.rand(size)\n",
    "        if initial_values is not None:\n",
    "            self.Q = initial_values\n",
    "        \n",
    "    def get_Q(self, state):\n",
    "        return self.Q[state]\n",
    "\n",
    "    def set_Q(self, state, value):\n",
    "        self.Q[state] = value\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "    def get_best_value(self, state):\n",
    "        return np.max(self.Q[state])\n",
    "\n",
    "    def get_Q_values(self):\n",
    "        return self.Q\n",
    "\n",
    "    def set_Q_values(self, values):\n",
    "        self.Q = values\n",
    "\n",
    "    def update(self, state, action, value):\n",
    "        self.Q[state, action] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 n_states, \n",
    "                 n_actions, \n",
    "                 rho = 0.9, \n",
    "                 alpha = 0.1, \n",
    "                 epsilon = 0.1):\n",
    "\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = Q_table((n_states, n_actions))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return self.Q.get_best_action(state)\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        target = reward + self.rho * self.Q.get_best_value(next_state)\n",
    "        error = target - self.Q.get_Q(state, action)\n",
    "        self.Q.update(state, action, self.Q.get_Q(state, action) + self.alpha * error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
